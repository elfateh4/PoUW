#!/usr/bin/env python3
"""
Enterprise Deployment Infrastructure Demo

This demo showcases the comprehensive enterprise deployment capabilities
of the PoUW system, including Kubernetes orchestration, production monitoring,
load balancing, auto-scaling, and infrastructure automation.
"""

import asyncio
import logging
import time
import json
from datetime import datetime, timedelta
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from pouw.deployment.kubernetes import (
        KubernetesOrchestrator,
        PoUWDeploymentManager,
        ContainerConfiguration,
        ServiceConfiguration,
        DeploymentStatus
    )
    from pouw.deployment.monitoring import (
        ProductionMonitor,
        MetricsCollector,
        AlertingSystem,
        LoggingManager,
        HealthChecker,
        PerformanceAnalyzer,
        Metric,
        Alert,
        AlertSeverity,
        MetricType
    )
    from pouw.deployment.infrastructure import (
        LoadBalancer,
        AutoScaler,
        InfrastructureAsCode,
        DeploymentAutomation,
        ConfigurationManager,
        ResourceManager,
        LoadBalancerConfig,
        LoadBalancingStrategy,
        AutoScalingRule,
        ScalingDirection
    )
except ImportError as e:
    logger.error(f"Failed to import PoUW deployment modules: {e}")
    logger.error("Make sure you're running from the project root directory")
    exit(1)


class EnterpriseDeploymentDemo:
    """Comprehensive demonstration of enterprise deployment capabilities"""
    
    def __init__(self):
        """Initialize demo components"""
        self.namespace = "pouw-demo"
        self.results = {}
        
        # Initialize components (simulation mode for demo)
        self.deployment_manager = PoUWDeploymentManager(namespace=self.namespace)
        # Create logs directory if it doesn't exist
        import os
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # Initialize with local log file
        log_file = log_dir / f"{self.namespace}.log"
        self.production_monitor = ProductionMonitor(namespace=self.namespace, log_file=str(log_file))
        self.config_manager = ConfigurationManager()
        self.resource_manager = ResourceManager()
        
        logger.info("Enterprise Deployment Infrastructure Demo initialized")
    
    async def run_complete_demo(self):
        """Run the complete enterprise deployment demo"""
        print("=" * 80)
        print("üöÄ PoUW Enterprise Deployment Infrastructure Demo")
        print("=" * 80)
        print()
        
        try:
            # Phase 1: Configuration Management
            await self.demo_configuration_management()
            
            # Phase 2: Container Orchestration
            await self.demo_kubernetes_orchestration()
            
            # Phase 3: Production Monitoring
            await self.demo_production_monitoring()
            
            # Phase 4: Load Balancing
            await self.demo_load_balancing()
            
            # Phase 5: Auto-scaling
            await self.demo_auto_scaling()
            
            # Phase 6: Infrastructure as Code
            await self.demo_infrastructure_as_code()
            
            # Phase 7: Resource Management
            await self.demo_resource_management()
            
            # Phase 8: Performance Analysis
            await self.demo_performance_analysis()
            
            # Final Summary
            self.print_demo_summary()
            
        except Exception as e:
            logger.error(f"Demo failed: {e}")
            raise
    
    async def demo_configuration_management(self):
        """Demonstrate configuration management capabilities"""
        print("üìã Phase 1: Configuration Management")
        print("-" * 50)
        
        # Get default configuration
        config = self.config_manager.get_default_deployment_config()
        print(f"‚úÖ Generated default deployment configuration with {len(config)} sections")
        
        # Save configuration
        config_name = "demo_deployment"
        success = self.config_manager.save_configuration(config_name, config)
        print(f"‚úÖ Saved configuration '{config_name}': {success}")
        
        # Load configuration
        loaded_config = self.config_manager.load_configuration(config_name)
        print(f"‚úÖ Loaded configuration: {loaded_config is not None}")
        
        # Validate configuration structure
        required_sections = ['config', 'secrets', 'load_balancer', 'scaling']
        valid_structure = all(section in config for section in required_sections)
        print(f"‚úÖ Configuration structure validation: {valid_structure}")
        
        self.results['configuration'] = {
            'config_sections': len(config),
            'save_success': success,
            'load_success': loaded_config is not None,
            'structure_valid': valid_structure
        }
        
        print(f"üìä Configuration Management Results:")
        for key, value in self.results['configuration'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_kubernetes_orchestration(self):
        """Demonstrate Kubernetes orchestration capabilities"""
        print("üê≥ Phase 2: Kubernetes Orchestration")
        print("-" * 50)
        
        # Get default configurations
        components = self.deployment_manager.get_default_configurations()
        services = self.deployment_manager.get_default_services()
        
        print(f"‚úÖ Generated {len(components)} component configurations")
        print(f"‚úÖ Generated {len(services)} service configurations")
        
        # Demonstrate container configuration
        blockchain_config = components['blockchain-node']
        print(f"‚úÖ Blockchain node configuration:")
        print(f"   ‚Ä¢ Image: {blockchain_config.image}:{blockchain_config.tag}")
        print(f"   ‚Ä¢ Ports: {blockchain_config.ports}")
        print(f"   ‚Ä¢ Environment variables: {len(blockchain_config.environment)}")
        
        # Convert to Kubernetes manifest
        manifest_dict = blockchain_config.to_dict()
        print(f"‚úÖ Generated Kubernetes container spec with {len(manifest_dict)} fields")
        
        # Service configuration
        api_service = services['blockchain-api']
        service_manifest = api_service.to_dict()
        print(f"‚úÖ Generated Kubernetes service spec: {service_manifest['kind']}")
        
        # Simulate cluster status
        cluster_status = await self.deployment_manager.get_cluster_status()
        print(f"‚úÖ Cluster status check: {cluster_status.get('health', 'unknown')}")
        
        self.results['kubernetes'] = {
            'components_count': len(components),
            'services_count': len(services),
            'manifest_fields': len(manifest_dict),
            'cluster_health': cluster_status.get('health', 'unknown')
        }
        
        print(f"üìä Kubernetes Orchestration Results:")
        for key, value in self.results['kubernetes'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_production_monitoring(self):
        """Demonstrate production monitoring capabilities"""
        print("üìä Phase 3: Production Monitoring")
        print("-" * 50)
        
        # Start monitoring services
        await self.production_monitor.start_monitoring()
        print("‚úÖ Started production monitoring services")
        
        # Simulate metrics collection
        metrics_collector = self.production_monitor.metrics_collector
        
        # Add sample metrics
        test_metrics = [
            Metric("cpu_usage", 45.5, MetricType.GAUGE, {"node": "worker-1"}),
            Metric("memory_usage", 67.2, MetricType.GAUGE, {"node": "worker-1"}),
            Metric("disk_usage", 34.8, MetricType.GAUGE, {"node": "worker-1"}),
            Metric("network_throughput", 125.7, MetricType.GAUGE, {"interface": "eth0"}),
            Metric("request_count", 1250, MetricType.COUNTER, {"endpoint": "/api/tasks"})
        ]
        
        for metric in test_metrics:
            metrics_collector.add_metric(metric)
        
        print(f"‚úÖ Collected {len(test_metrics)} sample metrics")
        
        # Get latest metrics
        latest_metrics = metrics_collector.get_latest_metrics()
        print(f"‚úÖ Retrieved {len(latest_metrics)} latest metrics")
        
        # Simulate alert generation
        alerting_system = self.production_monitor.alerting_system
        
        # Create test alert
        test_alert = Alert(
            id="demo_high_cpu",
            severity=AlertSeverity.WARNING,
            message="High CPU usage detected during demo",
            component="demo_system",
            metadata={"cpu_usage": 85.0}
        )
        
        await alerting_system._handle_alert(test_alert)
        print("‚úÖ Generated and handled test alert")
        
        # Get monitoring dashboard
        dashboard_data = self.production_monitor.get_monitoring_dashboard()
        print(f"‚úÖ Generated monitoring dashboard with {len(dashboard_data)} sections")
        
        # Stop monitoring
        await self.production_monitor.stop_monitoring()
        print("‚úÖ Stopped monitoring services")
        
        self.results['monitoring'] = {
            'metrics_collected': len(test_metrics),
            'latest_metrics': len(latest_metrics),
            'alerts_generated': 1,
            'dashboard_sections': len(dashboard_data)
        }
        
        print(f"üìä Production Monitoring Results:")
        for key, value in self.results['monitoring'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_load_balancing(self):
        """Demonstrate load balancing capabilities"""
        print("‚öñÔ∏è Phase 4: Load Balancing")
        print("-" * 50)
        
        # Create load balancer configurations
        lb_configs = {
            'round_robin': LoadBalancerConfig(
                name="pouw-api-rr",
                strategy=LoadBalancingStrategy.ROUND_ROBIN,
                backend_servers=[
                    {"host": "10.0.1.10", "port": 8080, "weight": 1},
                    {"host": "10.0.1.11", "port": 8080, "weight": 1},
                    {"host": "10.0.1.12", "port": 8080, "weight": 1}
                ],
                health_check={"path": "/health", "interval": 30}
            ),
            
            'weighted': LoadBalancerConfig(
                name="pouw-ml-weighted",
                strategy=LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN,
                backend_servers=[
                    {"host": "10.0.2.10", "port": 8080, "weight": 3},
                    {"host": "10.0.2.11", "port": 8080, "weight": 2},
                    {"host": "10.0.2.12", "port": 8080, "weight": 1}
                ]
            ),
            
            'least_conn': LoadBalancerConfig(
                name="pouw-vpn-lc",
                strategy=LoadBalancingStrategy.LEAST_CONNECTIONS,
                backend_servers=[
                    {"host": "10.0.3.10", "port": 1194},
                    {"host": "10.0.3.11", "port": 1194}
                ]
            )
        }
        
        print(f"‚úÖ Created {len(lb_configs)} load balancer configurations")
        
        # Generate nginx configurations
        nginx_configs = {}
        for name, config in lb_configs.items():
            nginx_config = config.to_nginx_config()
            nginx_configs[name] = nginx_config
            print(f"‚úÖ Generated nginx config for {name} strategy ({len(nginx_config)} bytes)")
        
        # Validate configurations contain required elements
        validation_results = {}
        for name, nginx_config in nginx_configs.items():
            validation_results[name] = {
                'has_upstream': 'upstream' in nginx_config,
                'has_servers': 'server 10.0.' in nginx_config,
                'has_proxy': 'proxy_pass' in nginx_config,
                'has_health_check': '/health' in nginx_config or name == 'least_conn'
            }
        
        print("‚úÖ Validated nginx configuration generation")
        
        # Simulate load balancer status
        for name, config in lb_configs.items():
            backend_count = len(config.backend_servers)
            healthy_backends = backend_count  # Simulate all healthy for demo
            print(f"   ‚Ä¢ {name}: {healthy_backends}/{backend_count} backends healthy")
        
        self.results['load_balancing'] = {
            'configurations_created': len(lb_configs),
            'nginx_configs_generated': len(nginx_configs),
            'validation_passed': all(
                all(v.values()) for v in validation_results.values()
            ),
            'total_backends': sum(len(config.backend_servers) for config in lb_configs.values())
        }
        
        print(f"üìä Load Balancing Results:")
        for key, value in self.results['load_balancing'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_auto_scaling(self):
        """Demonstrate auto-scaling capabilities"""
        print("üìà Phase 5: Auto-scaling")
        print("-" * 50)
        
        # Create mock orchestrator for demo
        class MockOrchestrator:
            def __init__(self):
                self.deployments = {
                    'blockchain-node': {'replicas': 3, 'ready_replicas': 3},
                    'ml-trainer': {'replicas': 2, 'ready_replicas': 2},
                    'vpn-mesh': {'replicas': 1, 'ready_replicas': 1}
                }
                self.scaling_actions = []
            
            async def get_deployment_status(self):
                return self.deployments
            
            async def scale_deployment(self, component, replicas):
                self.scaling_actions.append({'component': component, 'replicas': replicas})
                self.deployments[component]['replicas'] = replicas
                self.deployments[component]['ready_replicas'] = replicas
                return True
        
        mock_orchestrator = MockOrchestrator()
        auto_scaler = AutoScaler(mock_orchestrator)
        
        # Create scaling rules
        scaling_rules = {
            'blockchain-node': AutoScalingRule(
                metric_name="cpu_usage",
                threshold_up=75.0,
                threshold_down=25.0,
                min_replicas=2,
                max_replicas=8
            ),
            'ml-trainer': AutoScalingRule(
                metric_name="gpu_utilization",
                threshold_up=80.0,
                threshold_down=20.0,
                min_replicas=1,
                max_replicas=5
            )
        }
        
        for component, rule in scaling_rules.items():
            auto_scaler.add_scaling_rule(component, rule)
        
        print(f"‚úÖ Created {len(scaling_rules)} auto-scaling rules")
        
        # Simulate scaling scenarios
        scaling_scenarios = [
            {'component': 'blockchain-node', 'metric_value': 85.0, 'expected': 'scale_up'},
            {'component': 'blockchain-node', 'metric_value': 15.0, 'expected': 'scale_down'},
            {'component': 'ml-trainer', 'metric_value': 50.0, 'expected': 'maintain'},
            {'component': 'ml-trainer', 'metric_value': 90.0, 'expected': 'scale_up'}
        ]
        
        scaling_decisions = []
        for scenario in scaling_scenarios:
            # Mock metric retrieval
            auto_scaler._get_metric_value = lambda c, m: asyncio.coroutine(
                lambda: scenario['metric_value']
            )()
            
            rule = scaling_rules[scenario['component']]
            deployment_status = await mock_orchestrator.get_deployment_status()
            
            decision = await auto_scaler._evaluate_component_scaling(
                scenario['component'], rule, deployment_status
            )
            
            if decision:
                scaling_decisions.append({
                    'component': decision.component,
                    'direction': decision.direction.value,
                    'current': decision.current_replicas,
                    'target': decision.target_replicas,
                    'metric_value': decision.metric_value
                })
        
        print(f"‚úÖ Evaluated {len(scaling_decisions)} scaling scenarios")
        
        # Display scaling decisions
        for decision in scaling_decisions:
            direction = decision['direction']
            component = decision['component']
            current = decision['current']
            target = decision['target']
            metric = decision['metric_value']
            print(f"   ‚Ä¢ {component}: {direction} ({current}‚Üí{target} replicas, metric: {metric})")
        
        self.results['auto_scaling'] = {
            'rules_created': len(scaling_rules),
            'scenarios_evaluated': len(scaling_scenarios),
            'scaling_decisions': len(scaling_decisions),
            'scale_up_decisions': len([d for d in scaling_decisions if d['direction'] == 'up']),
            'scale_down_decisions': len([d for d in scaling_decisions if d['direction'] == 'down'])
        }
        
        print(f"üìä Auto-scaling Results:")
        for key, value in self.results['auto_scaling'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_infrastructure_as_code(self):
        """Demonstrate Infrastructure as Code capabilities"""
        print("üèóÔ∏è Phase 6: Infrastructure as Code")
        print("-" * 50)
        
        # Create IaC manager with temporary directory
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            iac = InfrastructureAsCode(project_dir=temp_dir)
            
            # Create deployment configuration
            deployment_config = {
                'config': {
                    'POUW_NETWORK_ID': '1337',
                    'POUW_LOG_LEVEL': 'info',
                    'POUW_METRICS_ENABLED': 'true',
                    'POUW_VPN_ENABLED': 'true'
                },
                'secrets': {
                    'POUW_SECRET_KEY': 'demo-secret-key',
                    'POUW_DB_PASSWORD': 'demo-db-password'
                }
            }
            
            # Generate Terraform configuration
            terraform_config = iac.generate_terraform_config(deployment_config)
            print(f"‚úÖ Generated Terraform configuration ({len(terraform_config)} bytes)")
            
            # Validate Terraform configuration content
            validation_checks = {
                'has_terraform_block': 'terraform {' in terraform_config,
                'has_providers': 'provider "kubernetes"' in terraform_config,
                'has_namespace': 'kubernetes_namespace' in terraform_config,
                'has_configmap': 'kubernetes_config_map' in terraform_config,
                'has_secrets': 'kubernetes_secret' in terraform_config,
                'has_monitoring': 'helm_release' in terraform_config,
                'has_outputs': 'output "namespace"' in terraform_config
            }
            
            validation_passed = all(validation_checks.values())
            print(f"‚úÖ Terraform configuration validation: {validation_passed}")
            
            # Count configuration elements
            config_elements = {
                'resources': terraform_config.count('resource "'),
                'providers': terraform_config.count('provider "'),
                'outputs': terraform_config.count('output "'),
                'variables': terraform_config.count('variable "')
            }
            
            print(f"‚úÖ Configuration contains {config_elements['resources']} resources")
            
            # Write configuration to file (simulated)
            config_file = Path(temp_dir) / "main.tf"
            with open(config_file, 'w') as f:
                f.write(terraform_config)
            
            print(f"‚úÖ Wrote Terraform configuration to {config_file.name}")
            
            self.results['infrastructure_as_code'] = {
                'config_size_bytes': len(terraform_config),
                'validation_passed': validation_passed,
                'resources_count': config_elements['resources'],
                'config_sections': len(deployment_config),
                'validation_checks_passed': sum(validation_checks.values())
            }
        
        print(f"üìä Infrastructure as Code Results:")
        for key, value in self.results['infrastructure_as_code'].items():
            print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_resource_management(self):
        """Demonstrate resource management capabilities"""
        print("üíæ Phase 7: Resource Management")
        print("-" * 50)
        
        # Collect current resource usage
        usage = await self.resource_manager.collect_resource_usage()
        print("‚úÖ Collected current system resource usage")
        
        # Display resource metrics
        cpu_usage = usage['cpu']['percent']
        memory_usage = usage['memory']['percent']
        disk_usage = (usage['disk']['used'] / usage['disk']['total']) * 100
        
        print(f"   ‚Ä¢ CPU Usage: {cpu_usage:.1f}%")
        print(f"   ‚Ä¢ Memory Usage: {memory_usage:.1f}%")
        print(f"   ‚Ä¢ Disk Usage: {disk_usage:.1f}%")
        
        # Simulate additional resource data for trend analysis
        import random
        for i in range(5):
            simulated_usage = {
                'timestamp': datetime.now().isoformat(),
                'cpu': {'percent': random.uniform(20, 80)},
                'memory': {'percent': random.uniform(30, 70)},
                'disk': {'used': usage['disk']['used'], 'total': usage['disk']['total']},
                'network': usage['network']
            }
            self.resource_manager.resource_usage['system'].append(simulated_usage)
        
        print("‚úÖ Simulated additional resource data for trend analysis")
        
        # Get resource recommendations
        recommendations = self.resource_manager.get_resource_recommendations()
        print(f"‚úÖ Generated {len(recommendations)} resource recommendations:")
        
        for i, recommendation in enumerate(recommendations, 1):
            print(f"   {i}. {recommendation}")
        
        # Resource utilization analysis
        usage_history = self.resource_manager.resource_usage['system']
        avg_cpu = sum(u['cpu']['percent'] for u in usage_history) / len(usage_history)
        avg_memory = sum(u['memory']['percent'] for u in usage_history) / len(usage_history)
        
        print(f"‚úÖ Average resource utilization over {len(usage_history)} samples:")
        print(f"   ‚Ä¢ Average CPU: {avg_cpu:.1f}%")
        print(f"   ‚Ä¢ Average Memory: {avg_memory:.1f}%")
        
        self.results['resource_management'] = {
            'current_cpu_usage': cpu_usage,
            'current_memory_usage': memory_usage,
            'current_disk_usage': disk_usage,
            'recommendations_count': len(recommendations),
            'usage_samples': len(usage_history),
            'average_cpu': avg_cpu,
            'average_memory': avg_memory
        }
        
        print(f"üìä Resource Management Results:")
        for key, value in self.results['resource_management'].items():
            if isinstance(value, float):
                print(f"   ‚Ä¢ {key}: {value:.1f}")
            else:
                print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    async def demo_performance_analysis(self):
        """Demonstrate performance analysis capabilities"""
        print("‚ö° Phase 8: Performance Analysis")
        print("-" * 50)
        
        performance_analyzer = PerformanceAnalyzer()
        
        # Record sample performance metrics
        performance_metrics = [
            ("api_response_time", 150.5, {"endpoint": "/api/tasks"}),
            ("api_response_time", 125.3, {"endpoint": "/api/tasks"}),
            ("api_response_time", 175.8, {"endpoint": "/api/health"}),
            ("database_query_time", 45.2, {"query": "SELECT"}),
            ("database_query_time", 52.1, {"query": "INSERT"}),
            ("ml_training_time", 3500.0, {"model": "SimpleMLP"}),
            ("ml_training_time", 3200.0, {"model": "SimpleMLP"}),
            ("network_latency", 25.5, {"destination": "peer_node"}),
            ("throughput", 1250.0, {"component": "blockchain"}),
            ("throughput", 1180.0, {"component": "blockchain"})
        ]
        
        for metric_name, value, metadata in performance_metrics:
            performance_analyzer.record_performance_metric(metric_name, value, metadata)
        
        print(f"‚úÖ Recorded {len(performance_metrics)} performance metrics")
        
        # Add analysis rules
        def response_time_analysis(data):
            recommendations = []
            insights = {}
            
            if 'api_response_time' in data:
                avg_response = data['api_response_time']['avg']
                if avg_response > 200:
                    recommendations.append("API response time is high. Consider optimizing database queries or adding caching.")
                elif avg_response < 100:
                    insights['api_performance'] = "API response time is excellent"
                
            if 'database_query_time' in data:
                avg_db_time = data['database_query_time']['avg']
                if avg_db_time > 50:
                    recommendations.append("Database query time is elevated. Consider indexing or query optimization.")
            
            return {'recommendations': recommendations, 'insights': insights}
        
        def throughput_analysis(data):
            recommendations = []
            insights = {}
            
            if 'throughput' in data:
                avg_throughput = data['throughput']['avg']
                if avg_throughput < 1000:
                    recommendations.append("System throughput is below optimal. Consider scaling up resources.")
                else:
                    insights['throughput_status'] = "System throughput is within normal range"
            
            return {'recommendations': recommendations, 'insights': insights}
        
        performance_analyzer.add_analysis_rule(response_time_analysis)
        performance_analyzer.add_analysis_rule(throughput_analysis)
        
        print("‚úÖ Added custom performance analysis rules")
        
        # Run performance analysis
        analysis_result = performance_analyzer.analyze_performance(timedelta(minutes=30))
        
        print(f"‚úÖ Completed performance analysis")
        print(f"   ‚Ä¢ Metrics analyzed: {analysis_result['metrics_analyzed']}")
        print(f"   ‚Ä¢ Recommendations: {len(analysis_result['recommendations'])}")
        print(f"   ‚Ä¢ Insights: {len(analysis_result['insights'])}")
        
        # Display recommendations
        if analysis_result['recommendations']:
            print("üìã Performance Recommendations:")
            for i, rec in enumerate(analysis_result['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        # Display insights
        if analysis_result['insights']:
            print("üí° Performance Insights:")
            for key, insight in analysis_result['insights'].items():
                print(f"   ‚Ä¢ {key}: {insight}")
        
        # Performance metrics summary
        aggregated_data = analysis_result['aggregated_data']
        metrics_summary = {}
        for metric_name, data in aggregated_data.items():
            metrics_summary[metric_name] = {
                'average': data['avg'],
                'min': data['min'],
                'max': data['max'],
                'samples': data['count']
            }
        
        self.results['performance_analysis'] = {
            'metrics_recorded': len(performance_metrics),
            'analysis_rules': 2,
            'metrics_analyzed': analysis_result['metrics_analyzed'],
            'recommendations_generated': len(analysis_result['recommendations']),
            'insights_generated': len(analysis_result['insights']),
            'metrics_summary': metrics_summary
        }
        
        print(f"üìä Performance Analysis Results:")
        for key, value in self.results['performance_analysis'].items():
            if key != 'metrics_summary':
                print(f"   ‚Ä¢ {key}: {value}")
        print()
    
    def print_demo_summary(self):
        """Print comprehensive demo summary"""
        print("=" * 80)
        print("üìä ENTERPRISE DEPLOYMENT INFRASTRUCTURE DEMO SUMMARY")
        print("=" * 80)
        print()
        
        # Calculate overall metrics
        total_components = (
            self.results['kubernetes']['components_count'] +
            self.results['kubernetes']['services_count']
        )
        
        total_configurations = (
            self.results['load_balancing']['configurations_created'] +
            self.results['auto_scaling']['rules_created']
        )
        
        total_metrics = (
            self.results['monitoring']['metrics_collected'] +
            self.results['performance_analysis']['metrics_recorded']
        )
        
        print("üéØ KEY ACHIEVEMENTS:")
        print(f"   ‚Ä¢ Kubernetes Components: {total_components}")
        print(f"   ‚Ä¢ Load Balancer Configurations: {self.results['load_balancing']['configurations_created']}")
        print(f"   ‚Ä¢ Auto-scaling Rules: {self.results['auto_scaling']['rules_created']}")
        print(f"   ‚Ä¢ Monitoring Metrics: {total_metrics}")
        print(f"   ‚Ä¢ Infrastructure Resources: {self.results['infrastructure_as_code']['resources_count']}")
        print(f"   ‚Ä¢ Performance Recommendations: {self.results['performance_analysis']['recommendations_generated']}")
        print()
        
        print("üèóÔ∏è INFRASTRUCTURE CAPABILITIES:")
        print(f"   ‚Ä¢ Kubernetes Orchestration: ‚úÖ {self.results['kubernetes']['components_count']} components")
        print(f"   ‚Ä¢ Production Monitoring: ‚úÖ {self.results['monitoring']['dashboard_sections']} dashboard sections")
        print(f"   ‚Ä¢ Load Balancing: ‚úÖ {self.results['load_balancing']['total_backends']} backend servers")
        print(f"   ‚Ä¢ Auto-scaling: ‚úÖ {self.results['auto_scaling']['scaling_decisions']} scaling decisions")
        print(f"   ‚Ä¢ Infrastructure as Code: ‚úÖ {self.results['infrastructure_as_code']['config_size_bytes']} bytes config")
        print(f"   ‚Ä¢ Resource Management: ‚úÖ {self.results['resource_management']['recommendations_count']} recommendations")
        print()
        
        print("üìà PERFORMANCE METRICS:")
        print(f"   ‚Ä¢ System CPU Usage: {self.results['resource_management']['current_cpu_usage']:.1f}%")
        print(f"   ‚Ä¢ System Memory Usage: {self.results['resource_management']['current_memory_usage']:.1f}%")
        print(f"   ‚Ä¢ System Disk Usage: {self.results['resource_management']['current_disk_usage']:.1f}%")
        print(f"   ‚Ä¢ Performance Metrics Analyzed: {self.results['performance_analysis']['metrics_analyzed']}")
        print()
        
        print("üîß CONFIGURATION MANAGEMENT:")
        print(f"   ‚Ä¢ Configuration Sections: {self.results['configuration']['config_sections']}")
        print(f"   ‚Ä¢ Save/Load Operations: ‚úÖ Successful")
        print(f"   ‚Ä¢ Structure Validation: ‚úÖ Passed")
        print()
        
        print("üöÄ DEPLOYMENT READINESS:")
        readiness_checks = [
            ("Kubernetes Orchestration", self.results['kubernetes']['components_count'] > 0),
            ("Load Balancing", self.results['load_balancing']['validation_passed']),
            ("Monitoring System", self.results['monitoring']['dashboard_sections'] > 0),
            ("Auto-scaling Rules", self.results['auto_scaling']['rules_created'] > 0),
            ("Infrastructure Code", self.results['infrastructure_as_code']['validation_passed']),
            ("Resource Management", self.results['resource_management']['recommendations_count'] > 0)
        ]
        
        passed_checks = sum(1 for _, passed in readiness_checks if passed)
        readiness_percentage = (passed_checks / len(readiness_checks)) * 100
        
        for check_name, passed in readiness_checks:
            status = "‚úÖ READY" if passed else "‚ùå NEEDS ATTENTION"
            print(f"   ‚Ä¢ {check_name}: {status}")
        
        print()
        print(f"üéä OVERALL DEPLOYMENT READINESS: {readiness_percentage:.0f}% ({passed_checks}/{len(readiness_checks)} checks passed)")
        
        if readiness_percentage == 100:
            print("üéâ CONGRATULATIONS! The PoUW Enterprise Deployment Infrastructure is PRODUCTION-READY!")
        else:
            print("‚ö†Ô∏è  Some components need attention before production deployment.")
        
        print()
        print("=" * 80)
        print("Demo completed successfully! üöÄ")
        print("=" * 80)


async def main():
    """Main demo execution function"""
    try:
        print("Initializing Enterprise Deployment Infrastructure Demo...")
        demo = EnterpriseDeploymentDemo()
        
        start_time = time.time()
        await demo.run_complete_demo()
        end_time = time.time()
        
        execution_time = end_time - start_time
        print(f"‚è±Ô∏è  Total execution time: {execution_time:.2f} seconds")
        
    except KeyboardInterrupt:
        print("\nüõë Demo interrupted by user")
    except Exception as e:
        logger.error(f"Demo failed with error: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
